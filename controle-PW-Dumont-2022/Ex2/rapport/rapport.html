<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Une appréhension globalisée du droit au respect de la vie privée et développement des IAs</title>
    <link rel="stylesheet" href="Index.html">
</head>
<body>
    <h2>
        I. Une appréhension globalisée du droit au respect de la vie privée et développement des IAs.
    </h2>
    <p>
        Au sein de sa restitution et après avoir exposé le fait que la vie privée est une donnée entretenant des liens
        importants avec les institutions étatiques, Maël Pégny évoque les enjeux de la collecte de données personnelles, à
        savoir les questions relatives à la collecte et la formation de connaissances (réelles ou prétendues) sur les individus.
        L’explosion de la collecte induit l’automatisation de l’analyse. On définit l’intelligence artificielle par son objectif :
        créer des systèmes informatiques intelligents en exécutant des tâches communément considérées comme
        intelligentes.</p>
        <p>
        Une fois ces définitions posées, Maël Pégny présente les problématiques relatives à « L’âge d’or de la
        surveillance
        ” : la surveillance de masse en passant par le capitalisme de la surveillance pour finir par la propagande
        de masse et le profilage des individus. Enfin ont été abordées les notions de mort de l’anonymat en ligne et les enjeux
        qu’elle soulève.
        S’appliquent à l’IA des prises de position fortes pour interdire certaines pratiques économiques manipulatoires,
        exploitatrices ou visant au contrôle social. L’IA fait également face à une exigence de transparence, en informant sur
        l’existence d’une interaction avec un robot ou sur la détection d’émotion. Maël Pégny soulève la question suivante :
        pourquoi ces restrictions s'appliquent- elles seulement à des systèmes qualifiés d’IA ?
        La reconnaissance faciale est l’exemple type d’intelligence artificielle. Elle regroupe deux formes d’outils : la
        reconnaissance à partir de photographies et la reconnaissance en direct à partir d’images de caméras de vidéo de 
        surveillance dans l’espace public (FRT). C’est cette seconde forme qui focalise le débat. En effet, elle marque une
        rupture juridique en appliquant à l’ensemble de la population des pratiques d’identification réservées aux suspects et
        criminels avérés, et une rupture symbolique : c’est la fin de l’anonymat dans l’espace public. Ainsi, il ne faut pas
        surestimer l’efficacité de la FRT, qui connaît énormément de dysfonctionnements avec un taux de réussite de 0% sur
        des tests menés en 2016. Cependant, cela pose un problème de fond majeur : que se passerait-il si la technique
        fonctionnait bien ? Deux écoles de pensée s’opposent. D’une part, l’école réglementaire qui défend un usage très
        limité de la reconnaissance faciale dans les cas de terrorisme ou de disparition d’enfant par exemple. 
        D’autre part, l’école prohibitionniste qui défend le fait qu’un tel dispositif ne devrait pas exister. En outre, le développement de ce
        type d’IA pose deux questions essentielles : la question du développement de la technologie et notamment des bases
        de données d’entraînement qui doivent être créées, et la question de l’usage de cette technologie et surtout du contrôle
        institutionnel.</p>
        On se rend compte de la nécessité de la création de vastes bases de données collectées dans des conditions
        naturelles pour l'entraînement des modèles. L’approche d’usage réglementé entérine la collecte de données à l’échelle
        d’une population sans consentement. Maël Pégny alerte quant à la perte de sensibilité contemporaine face à cette
        collecte et se pose la question de la fidélité à l’esprit originel du droit des données personnelles.
        Le développement de la FRT constitue en soi un tournant juridique et le problème du contrôle institutionnel est
        une question cruciale. Est-on capable de borner l’emploi d’un outil de contrôle social ? La question peut
        raisonnablement être soulevée au regard de certaines pratiques inquiétantes dans les polices de certains pays
        démocratiques, notamment aux Etats-Unis qui sont source d’inquiétude comme les identifications de manifestants
        “Black Lives Matter'', ou le programme FBI “COINTELPRO” dont l’objectif fut d'enquêter sur les organisations
        politiques dissidentes aux Etats-Unis et de perturber leurs activités. Dans la Résolution du Parlement européen du 20
        octobre 2020 concernant un cadre pour les aspects éthiques de l’intelligence artificielle, de la robotique et des
        technologies connexes (2020/2012(INL)), est interdite l’identification biométrique à distance dans l’espace public en
        temps réel à des fins répressives. Cependant, au regard du nombre d’exceptions, cela ressemble davantage à une
        autorisation plutôt qu'à une prohibition puisque sont listées toutes les infractions de droit commun.
        Toutes ces questions ont un impact conceptuel sur notre ordre légal et notamment sur les données personnelles.
        C’est une notion extrêmement large et sémantique. L’IA a pour conséquence de déduire des choses des données
        qu’on lui transmet : “Vous ne pouvez protéger ce que je peux déduire”. Ce n’est pas la donnée qu’il faut protéger,
        mais la connaissance qu’elle contient. Dans ce cadre, Maël Pégny alerte sur la nécessité d’une régulation de
        l’inférence statistique4
        et notamment sur la validité scientifique des inférences (droit à l’inférence raisonnable) et de
        la protection de la vie privée contre le contournement statistique. Aujourd’hui, il est possible de procéder à des
        inférences d’appartenance (déduction de l’appartenance d’un sujet à la base d’entrainement).
        Cependant, le problème prépondérant est que le droit des données de l’UE est centré sur les données avant
        traitement. Une donnée est personnelle si elle permet d’identifier une personne physique. Il y a une nécessité de
        clarifier le rapport entre traitement et statut de donnée personnelle. Ces problématiques sont mises en lumière par le
        Groupe de travail “article 29”5
        , sur la protection des données. Trois critères des données personnelles peuvent être
        retenus : contenu, intention, résultat. Avec l’exploitation des données environnementales pour influencer les
        personnes dans les smart cities6
        , tout peut devenir une donnée personnelle par intention ou par résultat.
        Enfin, Maël Pégny nous rappelle qu’il ne faut pas penser qu’en termes de barrières mais il s’agit plutôt de se poser
        la question suivante : comment réorienter l’appareil de connaissance numérique des individus vers des fins saines ?
        Quelle reconnaissance par les institutions les individus souhaitent-ils ?
        En effet, la reconnaissance des individus par les institutions peut être désirée. Ce fut le cas notamment des femmes
        amérindiennes qui habitaient sur les réserves. Une base centralisée est la première chose qui a été demandée pour
        leur permettre de jouir des mêmes droits que les femmes américaines, notamment en ce qui concerne les violences
        sexuelles. 
    </p>
   

    
</body>
</html>